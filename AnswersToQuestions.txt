
1) Why is it so important that adjacent free blocks not be left as such?
What would happen if they were permitted?
It is important, so we can correctly keep track of the available blocks of sequential memory.
Example: There are two adjacent blocks of size A and B.
We are looking for a block of size X=A+B.
If the adjacent blocks are not merged, we will not be able to find a fit for X,
since they are represented as two individual blocks.

2) Which function(s) need to be concerned about adjacent free blocks?
In our design, we only need to be concerned about free blocks in myfree(), when
we find the block that we want to free. Here, we find out, if any of it's
adjacent blocks are also free, in which case they should be merged.
This is actually the only case, where adjacent free blocks can occur, if we handle it this way.
Therefore, we don't need to be concerned about adjacent free blocks anywhere else.

3) Name one advantage of each strategy.
First:
It runs quickly since it doesn't need to go through the whole list every time.

Next:
It runs even quick than first, since it doesn't need to go through the whole list every time,
and because if we allocate a lot of blocks after each other, we don't need to iterate through the
same allocated blocks each time.

Best:
It minimizes how often the big blocks are split, which minimizes the number of failed allocations

Worst:
The average number of small blocks is smaller than in the other strategies, because
we rarely get small "remaining blocks" after splitting a block in two.



4) Run the stress test on all strategies, and look at the results (tests.out).
What is the significance of "Average largest free block"?
It is nice to have some very large blocks available, so we have the possibility
to allocate large sizes of memory. This helps in avoiding failed allocations of large blocks.

Which strategy generally has the best performance in this metric?
In the most tests, it is the "best" strategy,

Why do you think this is?
It only splits the largest blocks when it is absolutely necessary.



5) In the stress test results (see Question 4), what is the significance of
"Average number of small blocks"?
It indicates the number of very small blocks, that are generally not very useful, but
can be seen a "wasted memory", since you will likely not be able to allocate anything in them.

Which strategy generally has the best performance in this metric?
The "Worst" strategy generally has the smallest number in this metric, which is the best
since we want to minimize the number of small blocks.

Why do you think this is?
This is because we rarely get small "remaining blocks" after splitting a block in two.


6) Eventually, the many mallocs and frees produces many small blocks scattered
across the memory pool.  There may be enough space to allocate a new block, but
not in one place.  It is possible to compact the memory, so all the free blocks
are moved to one large free block.  How would you implement this in the system
you have built?

7) If you did implement memory compaction, what changes would you need to make
in how such a system is invoked (i.e. from a user's perspective)?

8) How would you use the system you have built to implement realloc?  (Brief
explanation; no code)

9) Which function(s) need to know which strategy is being used?  Briefly explain
why this/these and not others.

10) Give one advantage of implementing memory management using a linked list
over a bit array, where every bit tells whether its corresponding byte is
allocated.

